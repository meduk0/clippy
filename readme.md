# Clippy ğŸ§ ğŸ”

**Clippy** is a lightweight multimodal semantic search engine that allows users to perform similarity-based searches using either image or text queries. It leverages OpenCLIP for image embeddings and Sentence-Transformers for text embeddings. The app features a simple web interface powered by Flask and returns the most relevant images or text snippets from local datasets.

---

## ğŸš€ Features

- ğŸ” **Image-to-Image Search**: Upload an image and find visually similar images using CLIP-based embeddings.
- ğŸ“ **Text-to-Text Search**: Enter a text query and retrieve the most semantically similar paragraphs.
- ğŸ–¼ï¸ **Text-to-Image & Image-to-Text** (coming soon!): Cross-modal search functionality planned.
- ğŸ“ **Local Embedding Storage**: Uses `.npy` files for fast cosine similarity lookup.
- ğŸ§ª Optimized for experimentation, rapid prototyping, and local inference.

---

## ğŸ§± Project Structure

```txt
../clippy/
â”œâ”€â”€ clipextract.py              # OpenCLIP model for image embeddings
â”œâ”€â”€ semantic_search.py          # Sentence-Transformers model for text embeddings
â”œâ”€â”€ server.py                   # Backend Flask server
â”œâ”€â”€ static
â”‚Â Â  â”œâ”€â”€ clipfeature             # Image embedding vectors (.npy), generated by clipextract.py
â”‚Â Â  â”œâ”€â”€ images                  # Image dataset for similarity search
â”‚Â Â  â”œâ”€â”€ txt_embeddings          # Text embedding vectors (.npy), generated by semantic_search.py
â”‚Â Â  â”œâ”€â”€ uploaded                # Temporary upload storage for user-submitted images
â”‚Â Â  â””â”€â”€ wikipedia_paragraphs_train.json  # Raw text dataset
â”œâ”€â”€ templates
â”‚Â Â  â””â”€â”€ index3.html             # Web frontend template
â”œâ”€â”€ text_db_extract.py          # Extracts and prepares the text dataset
â”œâ”€â”€ img_down.py                 # Extracts and prepares the image dataset (download them for local use )
```
---
## ğŸ“š Datasets

### ğŸ–¼ï¸ Image Dataset

The image dataset is from [Unsplash Lite](https://unsplash.com/data), available at:

```
DOWNLOAD_URL = "https://unsplash-datasets.s3.amazonaws.com/lite/latest/unsplash-research-dataset-lite-latest.zip"
```

It contains a selection of curated images suitable for lightweight training and inference tasks (400mb) .

### ğŸ“„ Text Dataset

The text corpus is derived from the [agentlans/wikipedia-paragraphs](https://huggingface.co/datasets/agentlans/wikipedia-paragraphs) dataset. The file `wikipedia_paragraphs_train.json` contains sampled Wikipedia paragraphs used for semantic search.

---

## âš™ï¸ Requirements

- Python 3.8+
- [Hugging Face Transformers](https://github.com/huggingface/transformers)
- [OpenCLIP](https://github.com/mlfoundations/open_clip)
- Flask
- NumPy
- scikit-learn
- Pillow
- open
clone the repository
```bash
git clone https://github.com/meduk0/clippy
cd clippy
```
Install dependencies:
  first of all check the requirements.txt (choose either faiss-gpu or faiss-cpu)
```bash
python3.8 -m venv env  # creating a virtual environment
source env/bin/activate # activating the virtual environment
pip install -r requirements.txt
```
Enjoy!
---

## ğŸ§  How It Works

### ğŸ”¹ Image Embeddings

`clipextract.py` uses OpenCLIP to generate `.npy` files for each image in the dataset. These vectors are stored in `static/clipfeature`.

### ğŸ”¹ Text Embeddings

`semantic_search.py` uses Sentence-Transformers to encode paragraphs from the `wikipedia_paragraphs_train.json` file and saves them into `static/txt_embeddings`.

### ğŸ”¹ Web Interface

The Flask app (`server.py`) serves a simple frontend from `index3.html`, allowing users to search using an image or text prompt.

---

## ğŸ–¥ï¸ Running the App

```bash
python server.py
```

Then go to [http://localhost:5000](http://localhost:5000) in your browser.

---

## ğŸ“¦ TODO
- [ ] Responsive frontend improvements
- [ ] add windows support (linux only for now )

---

## ğŸ“„ License

MIT License. See [LICENSE](LICENSE) for more information.

---

Let me know if you want badges, deployment instructions (Docker, Hugging Face Spaces), or a section on contributing!
Also , facing any problem just open an issue and we will discuss stuff :)
